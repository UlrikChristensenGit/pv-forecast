from logic import parsing, logs, transformations
from logic.integrations.db import LocalDB
from logic.integrations.datalake import DataLake
import polars as pl
from azure.storage.filedatalake import DataLakeServiceClient
from azure.identity import DefaultAzureCredential
import xarray as xr
logger = logs.get_logger(__name__)



class ETL:

    def __init__(self):
        self.db = LocalDB(path="/home/uch/PVForecast/data/.log_db")
        self.data_lake = DataLake(
            storage_account_name="saenigmaarchivedev",
            container_name="analysis",
        )
        self.fs_client = self.data_lake.get_file_system_client(file_system="")

    
    def get_available_batches(self) -> pl.DataFrame:
        df = self.db.read("nwp_log")
        
        # only consider batches that are complete
        df = df.filter(pl.count("run_id").over("model_run_time_utc") == 61)
               
        return df

    def get_transformed_batches(self) -> pl.DataFrame:
        df = self.db.read("latest_nwp_log")
        
        return df

    def get_new_batches(self) -> pl.DataFrame:
        available_batches = self.get_available_batches()

        transformed_batches = self.get_transformed_batches()

        new_forecast_batches = available_batches.join(transformed_batches, on="run_id", how="anti")

        return new_forecast_batches


    def log_transformed_forecast(self, forecast: dict):
        df = pl.DataFrame(data=[forecast])
        self.db.upsert("nwp_log", df)

    def read_forecast(self, forecast: dict) -> xr.Dataset:
        file_client = self.fs_client.get_file_client(
            file_path=forecast["output_path"],
        )

        ds = xr.open_dataset(
            filename_or_obj=file_client.download_file().read()
        )

        return ds
    
    def write_forecast(self, ds: xr.Dataset):
        

        with self.fs_client.get_file_client(output_path) as file_client:
            with file_client.create_file() as f:
                ds.to_netcdf(f)

    def read_batch(self, batch: pl.DataFrame):
        ds_list = []
        for forecast in batch.iter_rows(named=True):
            ds = self.read_forecast(forecast)
            ds_list.append(ds)
        
        return transformations.nwp.combine_datasets(ds_list)

    def write_batch(self, ds: xr.Dataset):
        output_path = f"analysis/nwp/{model_run_time_utc}.nc"

        for ds in ds.groupby("time_utc"):
            

        with self.fs_client.get_file_client(output_path) as file_client:
            with file_client.create_file() as f:
                ds.to_netcdf(f)

    def run(self):
        new_batches = self.get_new_batches()

        for _, batch in new_batches.groupby("model_run_time_utc"):
            logger.info(f"Running ETL for batch '{model_run_time_utc}'")

            ds = self.



            self.log_transformed_forecast(forecast)

            logger.info(f"Downloaded {forecast['run_id']}")
